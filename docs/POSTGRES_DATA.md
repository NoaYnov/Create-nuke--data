# Migration des DonnÃ©es vers PostgreSQL

## ðŸ“‹ Vue d'ensemble

Toutes les donnÃ©es collectÃ©es sont maintenant sauvegardÃ©es dans PostgreSQL. Ce document explique comment fonctionne le systÃ¨me et comment migrer les donnÃ©es existantes.

## ðŸ—„ï¸ Structure de la Base de DonnÃ©es

### Tables PostgreSQL

1. **`daily_stats`** - Statistiques globales quotidiennes
   - `date`, `platform`, `total_downloads`, `followers`, `versions_count`

2. **`version_stats`** - Statistiques par version
   - `date`, `platform`, `version_name`, `version_number`, `downloads`, `date_published`

3. **`modpack_stats`** - Statistiques des modpacks
   - `date`, `platform`, `modpack_name`, `modpack_slug`, `downloads`, `followers`

## ðŸ”„ Flux de DonnÃ©es Actuel

### Collecte Automatique (via `collect_stats.py`)

Le script de collecte sauvegarde **TOUTES** les donnÃ©es dans PostgreSQL :

```
Modrinth API â†’ PostgreSQL (daily_stats + version_stats)
CurseForge API â†’ PostgreSQL (daily_stats + version_stats + modpack_stats)
```

### Fichiers CSV/JSON

Les fichiers CSV/JSON dans le dossier `data/` sont conservÃ©s pour :
- **Backup** - Copie de sauvegarde des donnÃ©es
- **CompatibilitÃ©** - Lecture par d'autres outils si nÃ©cessaire

## ðŸ“¥ Import des DonnÃ©es Historiques

Pour importer les donnÃ©es existantes dans les fichiers CSV/JSON vers PostgreSQL :

### Via Docker

```bash
# ExÃ©cuter le script d'import dans le conteneur
docker-compose exec streamlit-app python scripts/import_to_postgres.py
```

### En local

```bash
# Depuis la racine du projet
python scripts/import_to_postgres.py
```

## âœ… VÃ©rification des DonnÃ©es

### 1. VÃ©rifier les donnÃ©es dans PostgreSQL

```bash
# Se connecter au conteneur PostgreSQL
docker-compose exec postgres psql -U createnuclear -d createnuclear_stats

# Commandes SQL utiles :
SELECT COUNT(*) FROM daily_stats;
SELECT COUNT(*) FROM version_stats;
SELECT COUNT(*) FROM modpack_stats;

# Voir les derniÃ¨res entrÃ©es
SELECT * FROM daily_stats ORDER BY date DESC LIMIT 5;
SELECT * FROM modpack_stats ORDER BY date DESC LIMIT 10;
```

### 2. VÃ©rifier via le script d'initialisation

```bash
docker-compose exec streamlit-app python scripts/init_db.py
```

## ðŸ”§ Configuration

Les donnÃ©es vont automatiquement dans PostgreSQL grÃ¢ce Ã  :

1. **`DATABASE_URL`** dans `src/config.py` - URL de connexion
2. **`StatsDatabase`** dans `src/core/database.py` - Classe de gestion DB
3. **`collect_stats.py`** - Sauvegarde automatique lors de la collecte

## ðŸ“Š Utilisation dans Streamlit

Le dashboard Streamlit lit les donnÃ©es depuis PostgreSQL :

```python
# Les fonctions de cache chargent depuis PostgreSQL
@st.cache_data(ttl=CACHE_TTL)
def load_modrinth_stats():
    db = get_database()
    return db.get_daily_stats_history("modrinth")
```

## ðŸŽ¯ Que Faire Maintenant ?

### Ã‰tape 1 : Importer les DonnÃ©es Existantes (optionnel)

Si vous avez des donnÃ©es dans `data/curseforge_modpacks.csv` ou `.json` :

```bash
docker-compose exec streamlit-app python scripts/import_to_postgres.py
```

### Ã‰tape 2 : Lancer la Collecte de DonnÃ©es

```bash
# Via le dashboard Streamlit
# Cliquez sur le bouton "ðŸ”„ Run Data Collection"

# Ou manuellement via Docker
docker-compose exec streamlit-app python src/collectors/collect_stats.py
```

### Ã‰tape 3 : VÃ©rifier que Tout Fonctionne

```bash
# VÃ©rifier la base de donnÃ©es
docker-compose exec streamlit-app python scripts/init_db.py
```

## ðŸ“ Notes Importantes

- âœ… **Toutes les nouvelles donnÃ©es** vont automatiquement dans PostgreSQL
- âœ… **Les fichiers CSV/JSON** sont toujours crÃ©Ã©s comme backup
- âœ… **Le dashboard** lit depuis PostgreSQL (avec cache)
- âœ… **L'historique** est maintenu dans PostgreSQL pour les graphiques temporels

## ðŸš€ Performance

PostgreSQL offre :
- **RequÃªtes rapides** avec index optimisÃ©s
- **Historique complet** sans duplication
- **AgrÃ©gations** efficaces pour les graphiques
- **Concurrence** pour accÃ¨s simultanÃ©s

## âš ï¸ Backup

Pour sauvegarder la base de donnÃ©es :

```bash
# CrÃ©er un dump
docker-compose exec postgres pg_dump -U createnuclear createnuclear_stats > backup.sql

# Restaurer un dump
docker-compose exec -T postgres psql -U createnuclear createnuclear_stats < backup.sql
```

## ðŸ” Debugging

Si les donnÃ©es ne s'affichent pas :

1. VÃ©rifier la connexion DB :
   ```bash
   docker-compose logs streamlit-app | grep -i database
   ```

2. VÃ©rifier le contenu :
   ```bash
   docker-compose exec streamlit-app python scripts/init_db.py
   ```

3. VÃ©rifier les logs de collecte :
   ```bash
   docker-compose logs stats-collector
   ```
